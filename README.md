# SeguranÃ§a de Prompt em Modelos de LLM  
### Protegendo a InteligÃªncia Conversacional contra ManipulaÃ§Ãµes e Vazamentos

Este repositÃ³rio contÃ©m o desenvolvimento do projeto focado em seguranÃ§a de prompt em modelos de linguagem (LLMs). O objetivo Ã© implementar e avaliar um **pipeline de seguranÃ§a end-to-end**, capaz de mitigar ataques como *prompt injection*, *indirect injection*, vazamento de dados, abuso de permissÃµes e *denial-of-wallet*, com mÃ©tricas mensurÃ¡veis e alinhamento a normas como **AI Act**, **OWASP** e **ISO**.

---

## ğŸš€ Objetivo Geral
Construir e validar um protÃ³tipo de seguranÃ§a para aplicaÃ§Ãµes que utilizam LLMs, integrando controles tÃ©cnicos, governanÃ§a e geraÃ§Ã£o de evidÃªncias de conformidade.

---

## ğŸ‘¥ Autores

Projeto desenvolvido na disciplina de **SeguranÃ§a da InformaÃ§Ã£o (UFAPE)** por:  
Leonardo Nunes, AntÃ´nio Marcos, Ãlvaro Gueiros, Lucas William, Mauro VinÃ­cius e Vandielson TenÃ³rio.
