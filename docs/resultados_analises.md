Seção: Resultados e Análise
Nesta etapa, realizamos experimentos automatizados a partir da API desenvolvida, utilizando quatro cenários distintos: (i) Prompt Seguro, (ii) Prompt Injection 1, (iii) Prompt Injection 2 e (iv) Prompt Longo Demais. Para cada cenário foram executadas múltiplas requisições à API de chat, registrando-se métricas de segurança (taxa de detecção e falsos positivos) e de desempenho (latência média e throughput).

Os resultados consolidados mostram que a taxa de detecção foi de 100% (1.0) em todos os cenários. Isso indica que o firewall de LLM conseguiu identificar corretamente tanto os casos de prompt injection quanto o cenário de prompt extremamente longo. Ou seja, sempre que havia risco de ataque (cenários de prompt injection ou prompt acima do limite configurado), o sistema respondeu com bloqueio.

Por outro lado, observou-se um falso positivo no cenário de Prompt Seguro, em que um dos testes legítimos foi classificado como suspeito. Essa situação aparece na métrica de falsos positivos igual a 1.0 para esse cenário, enquanto os cenários de ataque apresentaram falsos positivos iguais a 0.0. Na prática, isso significa que o firewall está configurado de maneira conservadora, priorizando segurança em detrimento de uma pequena perda de usabilidade. Em um ambiente real, seria interessante ajustar as regras para reduzir falsos positivos mantendo, sempre que possível, a taxa de detecção próxima de 100%.

Em relação ao desempenho, a latência média variou aproximadamente entre 8,5 ms e 13,3 ms. O cenário de Prompt Seguro apresentou a maior latência, enquanto os cenários de prompt injection e prompt longo tiveram tempos de resposta levemente menores. Uma possível explicação é que, em alguns casos, o fluxo de bloqueio é mais simples do que o fluxo de geração de resposta completa pelo modelo, o que reduz o tempo de processamento.

O throughput ficou na faixa de 75 a 117 requisições por segundo, dependendo do cenário. De forma geral, os cenários com latência menor apresentaram maior throughput, o que é esperado. Esses valores indicam que, para o volume de requisições simulado nos testes, a solução é capaz de processar as entradas de forma eficiente, sem gargalos significativos.

Os dados detalhados e as estatísticas agregadas foram organizados em planilhas (CSV e Excel), além de visualizados por meio de gráficos de barras que comparam as métricas por cenário. Esses artefatos facilitam a interpretação dos resultados e servem como base para tomadas de decisão futuras, como o ajuste fino do firewall, calibração dos limiares de bloqueio e dimensionamento de infraestrutura.

De maneira geral, os experimentos indicam que a PoC atende ao objetivo principal de proteger a aplicação de LLM contra ataques de prompt injection e abusos de entrada, mantendo ao mesmo tempo um desempenho adequado. Como trabalhos futuros, recomenda-se ampliar o conjunto de cenários de teste, incluir ataques mais sofisticados e aplicar técnicas adicionais de análise de logs e monitoramento contínuo em produção.