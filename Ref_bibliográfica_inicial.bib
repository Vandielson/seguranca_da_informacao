
@inproceedings{sun_llm_2025,
	address = {New York, NY, USA},
	series = {{AIFE} '24},
	title = {{LLM} {Security} {Alignment} {Framework} {Design} {Based} on {Personal} {Preference}},
	isbn = {9798400710650},
	url = {https://doi.org/10.1145/3708394.3708396},
	doi = {10.1145/3708394.3708396},
	abstract = {large language models (LLMs) are widely used, the experts have raised significant concerns about their controllability, safety, and usability. However, the vulnerability and evaluation methods of alignment become the research focus. Although RLHF secure alignment techniques attempt to solve these problems, they still face challenges such as high labeling costs and wrong target generalization. This study explores future trends, benefits, and challenges of LLM security alignment technology. It proposes a security alignment framework for LLMs tailored to individual preferences. This framework employs algorithmic and data countermeasures to enhance the model's generalization performance, lower the costs associated with manual labeling, and improve LLMs' controllability, usability, and safety. This innovative approach provides useful implications for future secure alignment developments in LLMs.},
	booktitle = {Proceeding of the 2024 {International} {Conference} on {Artificial} {Intelligence} and {Future} {Education}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Zhendan and Zhao, Ruibin},
	year = {2025},
	keywords = {AI Alignment, LLM Privacy, LLM Trust Framework, Preference Alignment, RLHF},
	pages = {6--11},
}

@inproceedings{wu_llmapphub_2025,
	address = {New York, NY, USA},
	series = {{FSE} {Companion} '25},
	title = {{LLMAppHub}: {A} {Large} {Collection} of {LLM}-based {Applications} for the {Research} {Community}},
	isbn = {9798400712760},
	url = {https://doi.org/10.1145/3696630.3731439},
	doi = {10.1145/3696630.3731439},
	abstract = {With the increasing adoption of large language models (LLMs), a growing number of applications leveraging these models have emerged. However, LLM-based applications face significant security challenges. The OWASP Top 10 for LLMs has identified critical vulnerabilities, including prompt injections, data leakage, and unauthorized code execution. While previous research has explored specific security issues, these studies often focus on a narrow range of problems and a small number of applications, leaving gaps in our understanding of security threats in real-world LLM-based applications. To facilitate research in this field, an automated tool was developed to systematically collect LLM applications and construct a dataset, particularly focusing on independent applications that leverage LLM APIs or frameworks. Our dataset includes 33,001 repositories, providing a broad and representative foundation for security analysis. Both our dataset and tool are publicly available to support further research and foster advancements in the security and development of LLM-based applications.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Zixuan and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei},
	year = {2025},
	note = {event-place: Clarion Hotel Trondheim, Trondheim, Norway},
	keywords = {LLM-based applications, large language model},
	pages = {1254--1255},
}

@inproceedings{yarram_privacy-preserving_2025,
	title = {Privacy-{Preserving} {Healthcare} {Data} {Security} {Using} {Large} {Language} {Models} and {Adaptive} {Access} {Control}},
	doi = {10.1109/AIIoT65859.2025.11105296},
	abstract = {Protecting privacy-sensitive data in the digital healthcare sector is imperative due to the escalating threat of data breaches, unauthorized access, and cyberattacks. Traditional security systems, including rule-based and signature-based approaches, may struggle to adapt to evolving circumstances and manage high-risk situations. This study presents a security solution that uses adaptive access control systems and a Large Language Model (LLM) to protect sensitive patient information and electronic health records (EHRs). This method ensures HIPAA and GDPR compliance through context-aware risk assessment, anomaly detection, and query sanitization procedures, enhancing data security. Using a dynamic threat modeling technique, the proposed system identifies zero-day vulnerabilities and malicious behavior through real-time healthcare data transmissions. Comparative analyses show that the proposed LLM-based methodology outperforms traditional security techniques regarding accuracy, recall, precision, and F1 score. This improves threat detection rates and reduces false positives. The results demonstrate the effectiveness of LLM-based security solutions in securing medical records and ensuring patient privacy, thus providing robust and flexible protection.},
	booktitle = {2025 {IEEE} {World} {AI} {IoT} {Congress} ({AIIoT})},
	author = {Yarram, Srimaan and Dasari, Nagaraju and Seshagani, Sreedhar Babu and Ganguly, Priyam},
	month = may,
	year = {2025},
	keywords = {Access Control, Access control, Anomaly Detection, Anomaly detection, Data Security, Data security, Electronic medical records, Healthcare, LLM, Large language models, Privacy, Protection, Real-time systems, Risk management, Threat modeling},
	pages = {0854--0860},
}

@inproceedings{bunzel_compliance_2025,
	title = {Compliance {Made} {Practical}: {Translating} the {EU} {AI} {Act} into {Implementable} {Security} {Actions}},
	doi = {10.1109/RAIE66699.2025.00016},
	abstract = {The EU AI Act, along with emerging regulations in other countries, mandates that AI systems meet security requirements to prevent risks associated with AI misuse and vulnerabilities. However, for practitioners, defining and achieving a secure AI system is complex and context-dependent, posing challenges in understanding what actions they need to take and when they are sufficient. ISO/IEC TR 24028/29 and ENISA Securing Machine Learning Algorithms offer a comprehensive framework for AI security, aligning with the EU AI Act's requirements by addressing risks, threats, and mitigation strategies. However, for practical implementation, these reports lack hands-on guidance. Industry resources like the OWASP AI Exchange and OWASP LLM Top 10 fill this gap by providing accessible, actionable insights for securing AI systems effectively. This paper addresses the question of responsibility in AI risk mitigation, especially for companies utilizing pretrained or off-the-shelf models. We want to clarify how companies can practically comply with the upcoming ISO 27090 and ensure compliance with the EU AI Act through actionable security strategies tailored to this prevalent use case.},
	booktitle = {2025 {IEEE}/{ACM} {International} {Workshop} on {Responsible} {AI} {Engineering} ({RAIE})},
	author = {Bunzel, Niklas},
	month = apr,
	year = {2025},
	keywords = {Artificial Intelligence, Artificial intelligence, Companies, ISO Standards, Industries, Machine learning algorithms, Prevention and mitigation, Regularization of AI, Regulation, Risk mitigation, Security, Standardization, Standardization of AI, Trustworthy AI},
	pages = {69--73},
}

@inproceedings{rathod_privacy_2025,
	title = {Privacy and {Security} {Challenges} in {Large} {Language} {Models}},
	doi = {10.1109/CCWC62904.2025.10903912},
	abstract = {Large Language Models (LLMs) are at the forefront of artificial intelligence advancements, demonstrating exceptional capabilities in natural language understanding and generation across diverse domains such as healthcare, finance, and customer service. However, their deployment introduces substantial secu-rity and privacy risks, including prompt injection, data leakage, and unauthorized data disclosures. These vulnerabilities highlight the need for robust frameworks to safeguard sensitive data and prevent misuse. This paper provides a comprehensive analysis of the security and privacy challenges in LLMs, examines existing mitigation strategies such as intelligent LLM firewalls, differen-tial privacy, and OW ASP-based security principles, and discusses future directions for ethical and secure LLM deployment. By addressing these challenges in detail, we identify gaps in current practices and propose a roadmap for the secure and responsible deployment of LLMs in high-stakes applications. Our findings underscore the importance of tailored security frameworks and privacy-preserving techniques to ensure the ethical and reliable use of LLMs in sensitive environments. Additionally, this pa-per emphasizes the significance of a human-in-the-loop (HITL) approach to ensure accountability and accuracy, particularly in critical domains. The discussion extends to emerging technologies such as retrieval-augmented generation (RAG) and adaptive threat detection systems, which hold promise for enhancing the security and ethical deployment of LLMs.},
	booktitle = {2025 {IEEE} 15th {Annual} {Computing} and {Communication} {Workshop} and {Conference} ({CCWC})},
	author = {Rathod, Vishal and Nabavirazavi, Seyedsina and Zad, Samira and Iyengar, Sundararaja Sitharama},
	month = jan,
	year = {2025},
	keywords = {AI Ethics, Adaptive Security Frameworks, Artificial intelligence, Computational modeling, Data Leakage, Data Protection, Ethical Bias Mitigation, Ethics, Federated Learning, Firewall, Firewalls (computing), Healthcare AI, Human-in-the-Loop (HITL), Industries, Large language models, Large language models (LLM), Medical services, Natural language processing, OWASP, Privacy, Privacy-Preserving Computation, Security, Technological innovation, Threat Modeling, Threat assessment},
	pages = {00746--00752},
}
